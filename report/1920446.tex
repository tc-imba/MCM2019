%%
%% This is file `mcmthesis-demo.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% mcmthesis.dtx  (with options: `demo')
%%
%% -----------------------------------
%%
%% This is a generated file.
%%
%% Copyright (C)
%%     2010 -- 2015 by Zhaoli Wang
%%     2014 -- 2016 by Liam Huang
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%   http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 2005/12/01 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%%
%% The Current Maintainer of this work is Liam Huang.
%%
\documentclass{mcmthesis}
\mcmsetup{CTeX = false,   % 使用 CTeX 套装时，设置为 true
        tcn = 1920446, problem = C,
        sheet = true, titleinsheet = true, keywordsinsheet = true,
        titlepage = true, abstract = true}

\usepackage{palatino}
\usepackage{mwe}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage{indentfirst}
\usepackage{gensymb}
\usepackage[ruled,lined,commentsnumbered]{algorithm2e}
\usepackage{geometry}
% \geometry{left=2cm,right=2cm,top=2cm,bottom=2cm} %%页边距
% \usepackage{algorithm}  
\usepackage{algorithmicx}  
\usepackage{algpseudocode}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm 

\usepackage{minted}
\usemintedstyle{autumn}
\setminted{
    linenos,breaklines,tabsize=4,xleftmargin=1.5em,
    breaksymbol=\quad,fontfamily=courier
}

\setlength{\headheight}{14pt}
\author{}
% \usepackage{cmbright}

\usepackage[final]{pdfpages}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\begin{document}
\linespread{0.6}  %%行间距
\setlength{\parskip}{0.5\baselineskip}  %%段间距
\title{SOS: Spreading Model, Optimization, and Strategy Program for the Opioid Crisis}

\date{\today}
\begin{abstract}
	As the deteriorating Opioid Crisis overwhelms the United States, the DEA/National Forensic Laboratory Information System (NFLIS) are striving to control the nationwide spread of opioid overdose indidents. In this paper, we formuate a concrete and novel framework, named \textbf{SOS ( Spreading Model, Optimization, and Strategy Program for the Opioid Crisis)}, seeking for new strategies to combat the exacerbating opioid crisis.
	
	Based on the report provided, we create a Spreading Model of opioid incidents, focusing on the individual counties located in five U.S. states: Ohio (OH), Kentucky (KY), West Virginia (WV), Virginia (VA), and Pennsylvania (PA). We first get the longitude and latitude of every county and utilize a \textbf{Back-Propagation Neutral Network (BPNN)} to characterize the reported opioid and heroin incidents in and between the five states and their counties from 2010 to 2017. Based on this initial model, we next adopt \textbf{Outlier Detection} to identify some most possible locations where specific opioid accidents might have started, and the trending opioids crisis in the future of the five states. 
	
	Furthermore, we combine \textbf{Principal Component Analysis (PCA)} and \textbf{Deep Learning Approach Auto-encoder (AE)} techniques to analyze the U.S. Census Socio-economic data provided, aiming to reduce dimensions and find any important principal components. Then we use \textbf{Linear Regressions (LR)} to evaluate these principal components, proving that the data is effective for our model. Then we are able to add the transformed variables under the principal components axis into our Spreading Model for modification.
	
	Eventually, according to the results above, we figure out a comprehensive and feasible \textbf{Strategy Program} called \textbf{``SSD''}. The whole strategy program can be divided into three parts: \textbf{Supply Control, Spread Control, and Demand Control}. After being tested by our promoted model, these strategies are proven to be significantly effective in controlling and preventing the opioid crisis.
	
	\begin{keywords}
		SOS; Back-Propagation Neutral Network (BPNN); Outlier Detection; Principal Component Analysis (PCA); Deep Learning Approach Auto-Encoder; Linear Regression (LR);  Strategy Program ``SSD''
	\end{keywords}
\end{abstract}

\maketitle

\tableofcontents

\newpage

\section{Introduction}	
\subsection{Problem Background}
The Opioid Crisis means the rapid increase in the use of prescription and non-prescription opioid drugs in the United States beginning in the late 1990s and continuing throughout the past two decades~\cite{op}. Nowadays, it has developed into a nationwide crisis sweeping across the United States. For instance, using the data supplied with this problem, we analyze just heroin identification counts during years 2010-2017 in Figure \ref{situation}, which to some extent demonstrate the rampant drug problem in these five states, especially in Ohio (OH) and Pennsylvania (PA).See appendix \ref{code:map} for the complete code. 

\begin{figure}[htbp]
	\centering 
	\includegraphics[width=1\linewidth]{../figure/test.png}  
	\caption{Heroin identification counts in years 2010-2017 in each of the counties from five states: Ohio (OH), Kentucky (KY), West Virginia (WV), Virginia (VA), and Pennsylvania (PA).} 
	\label{situation}  
\end{figure}

Studies show that the increase in opioid overdose deaths has been dramatic, and opioids are now responsible for 49,000 of the 72,000 drug overdose deaths overall in the US in 2017~\cite{NIH}. The rate of prolonged opioid use is also increasing globally, threatening not only Americans' health but also the U.S. economy in many aspects. Consequently, president Donald Trump declared the country's opioid crisis a ``national emergency''~\cite{Trump}.

Currently, the U.S. government has payed great attention and taken a bunch of measures on this issue~\cite{CORRIGAN201844}. While the U.S. Centers for Disease Control (CDC) continue to fight the opioid overdose epidemic, simply enforcing existing laws is still a complex challenge for the Federal Bureau of Investigation (FBI), and the U.S. Drug Enforcement Administration (DEA), among others~\cite{intro}. Therefore, they need investigate the spread and characteristics of the opioids and heroin incidents in the United States, so that they can develop their strategies to better control and prevent the deteriorating opioids overdose situation.

\subsection{Our Work}
In this paper, we propose a novel framework, named SOS ( Spreading Model, Optimization, and Strategy Pragram for the Opioid Crisis), seeking for new strategies to combat the exacerbating opioid crisis. The framework of SOS is shown in Figure \ref{Framework}. 

\begin{figure}[htbp]
	\centering 
	\includegraphics[width=1\linewidth]{../figure/framework2.pdf} 
	\caption{Framework of our model SOS: Spreading Model, Optimization, and Strategy Program for the Opioid Crisis.} 
	\label{Framework}  
\end{figure}


We can divide our SOS framework in Figure  \ref{Framework}  into following steps:

\begin{itemize}
\item \textbf{Spreading Model}: Based on the report provided, we focus on the individual counties located in five U.S. states: Ohio (OH), Kentucky (KY), West Virginia (WV), Virginia (VA), and Pennsylvania (PA). We first get the longitude and latitude of every county and build a \textbf{Back-Propagation Neutral Network (BPNN) Model~\cite{NURCIN2017417}} to describe the spread and characteristics of the reported opioid incidents over time. With this model, we identify some possible locations where specific opioid use might have started in each of the five states.
\item \textbf{Optimization}: Furthermore, we analyze the U.S. Census Socio-Economic data provided with two techniques:   \textbf{Principal Component Analysis (PCA)} and \textbf{Learning Approach: Auto Encoder~\cite{NURCIN2017417}} ,  aiming to reduce the dimensionalities and find any important principal components. Then we make \textbf{Linear Regressions} to test the validity of these components, with which we add to our Spreading Model.
\item \textbf{Strategy Program for the Opioid Crisis}: Eventually, we identify a comprehensive and feasible Strategy Program for countering the opioid crisis. It contains three aspects: \textbf{Supply Control, Spread Control, and Demand Control(SSD)} After being tested by our model, these strategies are proven to be significantly effective in controlling the opioid crisis.
\end{itemize}

\section{Assumptions}\label{Sec-Assume}
First and foremost, we make some basic assumptions and explain their rationales.
	\paragraph{Assumption 1.}\itshape{Each state pays great attention to the drug cases and establishes common goals to overcome the opioid crisis.}
	
	\upshape This assumption is the premise of our work, because our goals as well as actions make sense only if every state strives to attack the drug problem.
	
	\paragraph{Assumption 2.}\itshape{The spread of the opioid and heroin incidents only takes place between two counties that are close to each other.}
	
	\upshape To simplify this problem, we only take short-distance spread between counties into consideration, and omit long-distance smuggling of opioid.
	
	\paragraph{Assumption 3.}\itshape{There will be no sudden enactment of strict laws or regulations on drug control.}
	
	\upshape The sudden changes of laws on drug controls will change the amount of the the cases during the period of time. There is no sign for changing of the laws, so that the assumption is reasonable. In that case, our data can be treated stable.
	
	\paragraph{Assumption 4.}\itshape{The drug identification data and the county location data are reliable to a certain extent.}
	
	\upshape Although the data can not be as complete as the fact and some statistical errors are inevitable, we make this assumption to reach one valid solution.

\section{Nomenclature}\label{Sec-Nomen}  %符号表
In this paper we use the nomenclature in Table \ref{tab:Nomen} to describe our model. Other symbols that are used only once will be described later.
\begin{table}[H]
    \centering
    \caption{Nomenclature.}
    \label{tab:Nomen}
    \begin{tabular}{|c|c|}
\hline
    	Symbol & Definition\\
\hline
	$A_x$ & The longitude of point A\\
	$A_y$ & The latitude of point A \\
    $D_{ij}$ &  The distance between any two state counties\\
    $R^2$ & The coefficient of determination\\
	$h_{w}$ & A vector function depends on the input-layer weights \\
	$y$ & Target result of the function $h_{w}$ \\
	$Err_k$ & The $k$th component of the error vector $y-h_{w}$\\
	$\Delta_k$ &  A modified error $Err _{k}\times g{}'\left ( in_{k} \right )$\\
	$w_{k}$ & A set of $p$-dimensional vectors of weights or loadings\\
	$\mathbf {t} _{(i)}$ & A new $m$-dimensional vector of principal component scores \\
	$\mathbf{X^TX}$ & A positive semidefinite matrix\\
	$\mathbf{W}$ & A $p$-by-$p$ matrix whose columns are the eigenvectors of $\mathbf{X^TX}$\\
    MLP	& The Multilayer Perception\\
    AE	& Auto Encoder\\
    PCA & Principal Component Analysis\\
    LR  & Linear Regression\\
    BPNN & Back-Propagation Neutral Network \\
\hline
    \end{tabular}
\end{table}    

\section{SOS: Spreading Model, Optimization, and Strategy Program for the Opioid Crisis} \label{Sec-Model}
In this section, we will discuss all details about our model \textbf{SOS}. Generally, this model consists of three parts: Spreading Model  for Opioid Incidents, Optimization with Socio-Economic Components, and Strategy Program for the Opioid Crisis.

\subsection{Data Pre-Processing and Model Evaluation Criteria}

\subsubsection{Geometric Location and Distances between the Counties}

To begin with, we consider the relative distance between any two counties as one of the factors that affect the spreading of Opioid Crisis. However, the data of geometry information of the counties listed is not given in the dataset. Thus, we write a short python program to get the geographical locations of all the counties in the above five states from the \textbf{Microsoft Bing Map API}~\cite{bing}, which is a tool used to search through OpenStreetMap data by name and address. The official coordinates of these 462 counties that we get are partly present as follows. See appendix \ref{code:geocode} for the detailed code. 
    
\begin{table}[H]
    \centering
     \caption{Part of the official coordinates of these 462 counties we get through Python from Microsoft Bing Map API.}
    \label{tab:geotable}
    \begin{tabular}{|c|c|c|c|}
        \hline
	State &	County &	latitude &	longitude \\ \hline
	KY &	ADAIR&	37.97293091	&-86.84214783\\
	KY&	ALLEN&	37.61523056&	-82.72395325\\
	KY&	ANDERSON&	37.01742172	&-86.79804993\\
	KY&	BALLARD&	38.23900986	&-85.7456665\\
	KY&	BARREN&	37.18073654&	-86.62342072\\
	KY&	BATH&	37.24637985&	-82.90135956\\
	KY& BELL&	36.73059845&   -83.67401123\\
	KY&	BOONE&	37.51216888&	-84.32032776\\
	KY&	BOURBON	&37.05149841&	-84.62284088\\
\hline
    \end{tabular}
\end{table}    
    


In that case, we can calculate the distance between each two counties. The earth is a near-standard ellipsoid with an equatorial radius of 6,378,140 km and a polar radius of 6,356,755 km, with an average radius of 6,371,004 km. If we assume that the earth is a perfect sphere, then its radius is the average radius of the earth, called R. If the zero degree longitude line is the basis, the surface distance between any two points on the earth's surface can be calculated based on the longitude and latitude of the two points. Let the longitude and latitude of point $A$ be $(A_x, A_y)$, and the longitude and latitude of point $B$ be $(B_x, B_y)$. Then, according to the Trigonometric Derivation, the equation \ref{1}and \ref{2} for calculating the distance ($D$) between two points can be obtained.

\begin{equation}
\label{1}
          C = \sin(A_y)\sin(B_y)\cos(A_x-B_x) + \cos(A_y)\cos(B_y),
\end{equation}

\begin{equation}
\label{2}
         D = R\arccos(C)\pi/180,
\end{equation}

    
    where $C$ is a temporary variable and $D$ is distance. Using the location data we got before, we can make a matrix that represents all the distance between any two counties in these five states.

\subsubsection{Count of Drug Report by State}\label{count-drug}

    Besides, we rank the number of opioid incidents of each drug in each state county, and then we get the Table \ref{tab:my_label}. Since there is no statistic significance for small amounts of data, we screen out significant data with statistically large amounts for the following modeling. See appendix \ref{code:count} for the complete code. 
    
\begin{table}[H]
    \centering  
    \caption{Part of ranking Table according to the number of specific opioid incidents in each state county, sorted in descending order.}
    \label{tab:my_label}
    \begin{tabular}{|c|c|c|}
        \hline
        State & Substance Name & Drug Reports \\\hline
        KY & Hydrocodone & 861 \\
        KY & Oxycodone & 838 \\
        VA & Oxycodone & 816 \\
        VA & Hydrocodone & 746 \\
        VA & Heroin & 716 \\
        OH & Heroin & 682 \\
        OH & Oxycodone & 682 \\
        OH & Hydrocodone & 659 \\
        KY & Buprenorphine & 642 \\\hline
    \end{tabular}
\end{table}    
    
    

\subsubsection{Model Evaluation Criteria:  Coefficient of Determination}
    
    In our model, we take  Coefficient of Determination as our Evaluation Criteria.In statistics, the coefficient of determination, denoted $R^2$ and pronounced ``R squared'', is the proportion of the variance in the dependent variable that is predictable from the independent variable(s) ~\cite{R}. 
    
    A data set has $n$ values marked $y_1$,...,$y_n$ (collectively known as $y_i$ or as a vector $y = [y_1,...,y_n]^T$), each associated with a predicted (or modeled) value $f_1$,...,$f_n$ (known as $f_i$, or sometimes $\hat{y_i}$, as a vector $f$).

    Define the residuals as $e_i = y_i$ - $f_i$ (forming a vector $\vec{e}$ ). $\bar{y}$ is the mean of the observed data: 
    $\bar{y}=\frac{1}{n}\sum_{i=1}^n y_{i}$ , then the variability of the data set can be measured using three sums of squares formulas:
    \begin{gather*}
        SS_{tot}=\sum _{i} \left ( y_i-\bar{y} \right )^2 ~,~
         SS_{reg}=\sum _{i} \left ( f_i-\bar{y} \right )^2 ~,~
       SS_{res}=\sum _{i} \left ( y_i-f_i \right )^2=\sum _{i}e_i^2.
     \end{gather*}
     
    Here we use the most general definition of the coefficient of determination as equation \ref{R^2}.
    \begin{equation}\label{R^2}
        R^2\equiv 1-\frac{SS_{res}}{SS_{tot}}
    \end{equation}
    
   The better the linear regression fits the data in comparison to the simple average, the closer the value of $R^{2}$ is to 1. A constant model that always predicts the expected value of $y$, disregarding the input features, would get a $R^2$ score of 0.

\subsection{ Spreading Model for Opioid Incidents }

\subsubsection{Introduction}    

In order to describe how the opioids identification counts spread in and between the five states, we build a Back-Propagation Neutral Network model, using distance matrix and last year's data as input. Back-Propagation is a supervised learning algorithm, for training Artificial Neural Networks, especially,  multi-layer networks ~\cite{AI}. The weights that the minimum error occurs is then considered to be a solution to the learning problem. 

However, the origin \textbf{BPNN} model is full-connected, which means, it doesn't take the distance between counties into consideration. The connections between two counties far away from each other can have a great impact on the learning time and effect of the model. Thus, we use a sparse-connected hidden layer between the input layer and output layer to solve this problem. Whether two neutrons are connected depends on whether the distance of the two counties they represent is under a certain threshold. Two neutrons of same counties are connected since their distance is zero.

In addition, the drugs reports can not only depend on the data of last year, but the data several years before as well. In practice, we found that using data of the previous two years could result in the better performance of the model. So we use a linear combination of them can try to learn two parameters for both years.

Here is an example structure of our model. Suppose there are five counties, named $A$, $B$, $C$, $D$, $E$, connected as shown in Figure \ref{fig:example}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
    \node (A) at (-2,0) {A};	
    \node (B) at (0,1) {B};	
    \node (E) at (2,0) {E};	
    \node (D) at (0,-1) {D};	
    \node (C) at (0,0) {C};	
    \draw (A) -- (B) -- (E) -- (D) -- (A);
    \draw (A) -- (C) (B) -- (C) (D) -- (C) (E) -- (C);
    \end{tikzpicture}
    \caption{Diagrammatic drawing of five counties in our model}
    \label{fig:example}
\end{figure}

Then we can build a neutral network as shown in Figure \ref{fig:BPNN}. See appendix \ref{code:bpnn} for the complete python code. 
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>/.tip={Latex}]
    \foreach \i/\j in {1/A,2/B,3/C,4/D,5/E} {
    	\foreach \k in {1,2,3} {
    		\node [circle, draw] (\j\k) at (3*\k,-2*\i) {\j};	
    	}
    	\path[->] (\j1)++(left:2cm) edge (\j1.west);
    	\path[<-] (\j3)++(right:2cm) edge (\j3.east);
    	\draw (\j1) -- (\j2) -- (\j3);
    }
    \draw (A1) -- (B2)  (A1) -- (D2)  (A1) -- (C2);
    \draw (B1) -- (A2)  (B1) -- (E2)  (B1) -- (C2);
    \draw (E1) -- (B2)  (E1) -- (D2)  (E1) -- (C2);
    \draw (D1) -- (A2)  (D1) -- (E2)  (D1) -- (C2);
    \draw (C1) -- (A2)  (C1) -- (B2)  (C1) -- (E2)  (C1) -- (D2);
    \draw (A3) -- (B2)  (A3) -- (D2)  (A3) -- (C2);
    \draw (B3) -- (A2)  (B3) -- (E2)  (B3) -- (C2);
    \draw (E3) -- (B2)  (E3) -- (D2)  (C3) -- (C2);
    \draw (D3) -- (A2)  (D3) -- (E2)  (D3) -- (C2);
    \draw (C3) -- (A2)  (C3) -- (B2)  (C3) -- (E2)  (C3) -- (D2);
    \node (l1) at (3,-1) {Input Layer};
    \node (l1) at (6,-1) {Hidden Layer};
    \node (l1) at (9,-1) {Output Layer};
    \end{tikzpicture}
    \caption{Diagrammatic drawing of BPNN model}
    \label{fig:BPNN}
\end{figure}


\subsubsection{Back-Propagation Neutral Network (BPNN) Model} \label{Sec:BPNN} 
	
	
	Let $h_{w}$ be a vector function and $y$ be its target result. Then $Err_{k}$ be the $k$th component of the error vector $y-h_{w}$.
	
	Whereas a percentage network decomposes into $m$ seperate learning problems for an $m$-output problem, this decomposition fails in multi-layer network. The vector $h_{w}$ that returns depends on the all of the input-layer weights, so updates to those weights will depend on errors in the vector. Fortunately, this dependency is very simple in the case of any loss function that is additive across the components of the error vector $y-h_{w}$. For the $L_{2}$ loss, we have, for any weight $w$,  equation \ref{Loss}.
	\begin{equation}
	\label{Loss}
	   \frac{\partial }{\partial w}Loss\left ( w \right )=\frac{\partial }{\partial w}\left | y-h_{w}\left ( x \right ) \right |^{2}
   =\frac{\partial }{\partial w}\sum_{k}\left ( y_{k}-a_{k} \right )^{2}=\sum_{k}\frac{\partial }{\partial w}\left ( y_{k}-a_{k} \right )^{2} 
	\end{equation}

	The major complication comes from the addition of hidden layers to the network. Whereas the error ${y-h_{w}}$ at the output layer is clear, the error at the hidden layers seems mysterious because the training data do not say what value the hidden nodes should have. Fortunately, it turns out that we can back-propagate the error from the output layer to the hidden layers. The back-propagation progress emerges directly from a derivation of the overall error gradient.
	
	We can easily define a modified error $\Delta _{k}=Err _{k}\times g{}'\left ( in_{k} \right )$, where $in_{k}$ means the $k$th component of input, so that the weight update rule becomes the equation \ref{weight}.
	\begin{equation}
       \label{weight}
	    w_{j,k}\leftarrow w_{j,k}+\alpha \times a_{j} \times\Delta _{k}
	\end{equation}
	
	To update the connections between the input units and the hidden units, we need to define a quantity analogous to the error term for output nodes. Here is where we do the error back-propagation. The idea is that hidden node $j$ is ``responsible'' for some fraction of the error $\Delta _{k}$ in each of the output nodes to which it connects. Thus, the $\Delta _{k}$ values are divided according to the strength of the connections between the hidden node and the output node and are propagated back to provide the $\Delta_{j}$ values for the hidden layer. The propagation rule for the $\Delta$ values is showed in equation \ref{prop}.
	\begin{equation}
        \label{prop}
	    \Delta_{j} = g{}'\left ( in_{k} \right)\sum_{k}w_{j,k}\Delta_{k}
	\end{equation}

	Now the weight-update rule for the weights between the inputs and the hidden layer is essentially identical to the update rule for the output layer:
	\begin{equation}
	    w_{j,k}\leftarrow w_{j,k}+\alpha \times a_{j} \times\Delta _{k}
	\end{equation}
	
% 	The process of back-propagation can be shown as the following steps:
% \begin{itemize}
% \item Compute the $\Delta$ values for the output units, using the observed error.
% \item Starting with output layer, repeat the following for each layer in the network, until the earliest hidden layer is reached:
% \begin{itemize}
%     \item[-] Propagation the $\Delta$ values back to the previous layer.
%     \item[-] Update the weights between the two layers.
% \end{itemize}
% \end{itemize}

% From above, we compute the gradient for $Loss_{k}=\left ( y_{k}-a_{k} \right )^{2}$ at the $k$th output. The gradient of this loss with respect to weights connecting the hidden layer to the output layer will be zero except for weights $w_{j,k}$ that connect to the $k$th output unit. For those weights, we have the following equation \ref{align}.

% \begin{align}
% \label{align}
% \frac{\partial Loss_{k}}{\partial w_{j,k}}
% &=-2\left ( y_{k}-a_{k} \right )\frac{\partial a_{k}}{\partial w_{j,k}}=-2\left ( y_{k}-a_{k} \right )\frac{\partial g\left ( in_{k} \right )}{\partial w_{j,k}} \notag\\
% &=2\left ( y_{k}-a_{k} \right )g{}'\left ( in_{k} \right )\frac{\partial in_{k}}{\partial w_{j,k}} = 2\left ( y_{k}-a_{k} \right )g{}'\left ( in_{k} \right )\frac{\partial }{\partial w_{j,k}} \left ( \sum_{j} w_{j,k}a_{j}\right )\notag\\
% &=-2\left ( y_{k}-a_{k} \right )g{}'\left ( in_{k} \right )a_{j}=-a_{j}\Delta _{k}
% \end{align}

% To obtain the gradient with respect to the $w_{i,j}$ weights connecting the input layer to the hidden layer, we have to expand out the activation $a_{j}$ and reapply the chain rule. We will show the derivation in gory detail because it is interesting to see how the derivation operator propagates back through the network as equation \ref{omg}.

% \begin{align}
% \label{omg}
% \frac{\partial Loss_{k}}{\partial w_{i,j}} &=-2\left ( y_{k}-a_{k} \right )\frac{\partial a_{k}}{\partial w_{i,j}}=-2\left ( y_{k}-a_{k} \right )\frac{\partial g\left ( in_{k} \right )}{\partial w_{i,j}} \notag\\
% &=-2\left ( y_{k}-a_{k} \right )g{}'\left ( in_{k} \right ) \frac{\partial in_{k}}{\partial w_{i,j}} = -2\Delta_{k}\frac{\partial }{\partial w_{i,j}} \left ( \sum_{j} w_{j,k}a_{j}\right )\notag\\ &=-2\Delta_{k}w_{j,k}\frac{\partial a_{j}}{\partial w_{i,j}}=-2\Delta_{k}w_{j,k}\frac{\partial g\left ( in_{j} \right )}{\partial w_{i,j}}=-2\Delta_{k}w_{j,k}g{}'\left ( in_{j} \right )\frac{\partial in_{j}}{\partial w_{i,j}}\notag\\ &=-2\Delta_{k}w_{j,k}g{}'\left ( in_{j} \right )\frac{\partial }{\partial w_{i,j}}\left ( \sum_{j} w_{i,j}a_{i}\right )\notag\\ 
% &=-2\Delta_{k}w_{j,k}g{}'\left ( in_{j} \right )a_{i} = -a_{i}\Delta_{j}
% \end{align}

%  Thus, we obtain the update rules obtained earlier from intuitive considerations. It is also clear that the process can be continued for networks with more than one hidden layer.

% The error value indicates how much the network's output (actual) is off the mark from the expected output (target) so that we use the \textbf{Mean Squared Error Function} to calculate the error. In this way, we solve every individual differential to get the initial desired differential.

% We just saw how back propagation of errors is used in The Multilayer Perception (MLP) neural networks to adjust weights for the output layer to train the network. We use a similar process to adjust weights in the hidden layers of the network which we would see next with a real neural network's implementation since it will be easier to explain it with an example where we have actual numbers to play with.

\subsubsection{Prediction and Outlier Detection}

Outlier is a value that lies in a data series on its extremes, which is either very small or large and thus can affect the overall observation made from the data series. Outliers are also termed as extremes because they lie on the either end of a data series. We apply Outlier to look for the prediction of the the specific concerns and the source of the each drug crisis.  

Let $n$ be the number of data values in the data set. The Lower Quartile $Q1$ is the median of the lower half of the data set. The Upper Quartile $Q3$ is the median of the upper half of the data set. The Interquartile range $IQR$ is the spread of the middle 50\% of the data values.

\begin{equation}
\begin{aligned}
    IQR &= Q3-Q1 \\
    Lower Limit &= Q1-1.5IQR \\
    Upper Limit &= Q3+1.5IQR \\
\end{aligned}
\end{equation}

Therefore, any value that will be more than the upper limit or lesser than the lower limit will be the outliers. We identify that these points are trending to have an opioid crisis.

\subsubsection{Source Deduction and Future Prediction: Spreading Model between Counties}
\label{sec: Implementation}

As we mentioned before in \ref{count-drug} Table \ref{tab:my_label}, Hydrocodone in the state KY and Buprenorphine in OH are the most often occurred cases by statistics. So we choose it as a sample for us to derive its source. Considering that we have just 8-year data, so we only predict 2 years forward and backward to ensure our results accurate. Thus, With data from 2010 to 2017 inputted into our model, we predict how the identification counts for the Hydrocodone cases distribute in KY in 2008, which is shown in Table \ref{tab:2008} and Figure \ref{KY_Hydrocodone_5}. Similarly, we can predict Buprenorphine cases in Ohio (OH) in 2019, which is shown in Table \ref{2019} and Figure \ref{OH}.

\begin{table}[H]
 \caption{Source Dedection and Future Prediction}
  \label{tbl:main}
  \begin{subtable}[t]{0.45\linewidth}
    \centering
     \caption{Source Dedection: Hydrocodone identification counts in 2008 in  Kentucky (KY).}
    \label{tab:2008}
    \begin{tabular}{|c|c|}
        \hline
        County  &  Drug Reports  \\\hline
        JEFFERSON &  1255.26\\
        LAUREL &  118.82\\
        PERRY & 112.61\\
        BELL &  87.37\\
        HARLAN & 88.33\\
        PULASKI &  85.35\\
        FAYETTE &  115.80\\
        WHITLEY &  64.73\\
        PIKE &  91.37\\
        FLOYD &  70.30\\
        KNOX & 61.78\\
        \hline
    \end{tabular}
   

  \end{subtable}\hfill
  \begin{subtable}[t]{0.45\linewidth}
    \centering
    \caption{Future Prediction: Buprenorphine identification counts in 2019 in Ohio (OH).}
    \label{2019}
    \begin{tabular}{|c|c|}
        \hline
        County &   Drug Reports\\\hline       
        FRANKLIN & 240.30\\
        HAMILTON & 155.90\\
        MONTGOMERY & 140.91\\
        CUYAHOGA & 139.50\\
        FAIRFIELD & 98.06\\
        CRAWFORD & 89.35\\
        STARK & 82.62\\
        GUERNSEY & 81.52\\
        LAKE & 78.01\\
        COLUMBIANA &76.87\\
        BUTLER &73.77\\
        \hline
    \end{tabular}
  \end{subtable}
\end{table}


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figure/KY_Hydrocodone_5}  
	\subcaption{Hydrocodone identification counts in 2008 in  Kentucky (KY)} 
	\label{KY_Hydrocodone_5}  
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figure/OH_Buprenorphine_1}  
	\subcaption{ Buprenorphine identification counts in 2019 in Ohio (OH)} 
	\label{OH}
    \end{subfigure}
    \caption{ Source Dedection and Future Prediction  }\label{fig:fancy}
\end{figure}



Then we overlay such images generated from 2008 to 2012 in chronological order to show the spreading characteristics of Hydrocodone identification counts in Kentucky as Figure \ref{KY_Hydrocodone} shows. With this method, we identify some  possible locations where specific opioid use might have started in each of the five states. Similarly, we can demonstrate the spreading characteristics of Buprenorphine identification counts in Ohio as Figure \ref{OH_Buprenorphine} shows.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figure/KY_Hydrocodone} 
	\caption{Hydrocodone identification counts spreading diagram from 2008 to 2012 in  Kentucky} 
	\label{KY_Hydrocodone}  
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figure/OH_Buprenorphine} 
	\caption{Buprenorphine identification counts spreading diagram from 2014 to 2019 in Ohio} 
	\label{OH_Buprenorphine}  
    \end{subfigure}
    \caption{ Source Dedection and Future Prediction model }\label{fig:cool}
\end{figure}

\subsubsection{Source Deduction and Future Prediction: Spreading Model of States}

The Spreading Model of states can be easily modified to obtain source deduction and future prediction for each state, since the five states in the dataset are next to each other, we can consider they are full-connected in the neutral network. After removing the distance module in the previous model, we build a Spreading Model of states. See appendix \ref{code:bpnn} for the complete python code (same file as the previous model). 

For example, we take 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../figure/state_Buprenorphine} 
    \caption{Buprenorphine identification counts of five states from 2008 to 2019}
    \label{fig:state_Buprenorphine}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/state_Buprenorphine} 
    \caption{Buprenorphine identification counts of five states in each year from 2008 to 2019}
    \label{fig:state_Buprenorphine}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/OH_Buprenorphine} 
	\caption{  identification counts of five states in each year from 2008 to 2019} 
	\label{fig:}  
    \end{subfigure}
    \caption{  }
    \label{fig:cool}
\end{figure}

\subsection{Optimization: Modification using Socio-economic Component}\label{Sec:Optimazation}
\subsubsection{Dimensionality Reduction}

\paragraph{\textbf{Technique 1: Principal Component Analysis (PCA)}}

~\smallskip

Now, our model can basically describe the spread and characteristics of the opioid and identify some possible locations where specific opioid use might have started. Next, we consider whether any important factors from the U.S. Census socio-economic data provided can further modify our model so that it can explain how opioid use got to its current level, what contributes to the growth in opioid addiction, and analyze who is using/abusing it despite its known dangers.

When we process the U.S. Census socio-economic data, we first adopt  the Principal component analysis (PCA), which is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. If there are $n$ observations with $p$ variables, then the number of distinct principal components is $\min(n-1,p)$. 

PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

Consider a data matrix, $\mathbf{X}$, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor).

Mathematically, the transformation is defined by a set of p-dimensional vectors of weights or loadings $ \mathbf {w} _{(k)}=(w_{1},\dots ,w_{p})_{(k)}$ that map each row vector $\mathbf{x}_{(i)}$ of X to a new vector of principal component scores $ \mathbf {t} _{(i)}=(t_{1},\dots ,t_{m})_{(i)}$, given by
$$ {t_{k}}_{(i)}=\mathbf {x} _{(i)}\cdot \mathbf {w} _{(k)}\qquad \mathrm {for} \qquad i=1,\dots ,n\qquad k=1,\dots ,m$$
in such a way that the individual variables $ t_{1},\dots ,t_{m}$ of t considered over the data set successively inherit the maximum possible variance from $\mathbf {x}$, with each loading vector $\mathbf {w}$ constrained to be a unit vector. \\

In order to maximize variance, the first loading vector $\mathbf{w}_{(1)}$ thus has to satisfy
\begin{equation}
    \mathbf {w} _{(1)}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\left\{\sum _{i}\left(t_{1}\right)_{(i)}^{2}\right\}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\left\{\sum _{i}\left(\mathbf {x} _{(i)}\cdot \mathbf {w} \right)^{2}\right\}
\end{equation}
Equivalently, writing this in matrix form gives
\begin{equation}
    \mathbf {w} _{(1)}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\{\Vert \mathbf {Xw} \Vert ^{2}\}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\left\{\mathbf {w} ^{T}\mathbf {X} ^{T}\mathbf {Xw} \right\}
\end{equation} 

Since $\mathbf{w}_{(1)}$ has been defined to be a unit vector, it equivalently also satisfies
\begin{equation}
    \mathbf {w} _{(1)}={\operatorname {\arg \,max} }\,\left\{{\frac {\mathbf {w} ^{T}\mathbf {X} ^{T}\mathbf {Xw} }{\mathbf {w} ^{T}\mathbf {w} }}\right\}
\end{equation}

The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as $\mathbf{X^TX}$ is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector.

With $\mathbf{w}_{(1)}$ found, the first principal component of a data vector $\mathbf{x}_{(1)}$ can then be given as a score $t_{1(1)} = \mathbf{x}_{(1)} \cdot \mathbf{w}_{(1)}$ in the transformed co-ordinates, or as the corresponding vector in the original variables, $\{\mathbf{x}_{(1)} \cdot \mathbf{w}_{(1)}\} \mathbf{w}_{(1)}$. \\

The $k$th component can be found by subtracting the first $k_1$ principal components from $\textbf{X}$:
\begin{equation}
    \mathbf {\hat {X}} _{k}=\mathbf {X} -\sum _{s=1}^{k-1}\mathbf {X} \mathbf {w} _{(s)}\mathbf {w} _{(s)}^{\rm {T}}
\end{equation}

and then finding the loading vector which extracts the maximum variance from this new data matrix
\begin{equation}
    \mathbf {w} _{(k)}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {arg\,max} }}\left\{\Vert \mathbf {\hat {X}} _{k}\mathbf {w} \Vert ^{2}\right\}={\operatorname {\arg \,max} }\,\left\{{\tfrac {\mathbf {w} ^{T}\mathbf {\hat {X}} _{k}^{T}\mathbf {\hat {X}} _{k}\mathbf {w} }{\mathbf {w} ^{T}\mathbf {w} }}\right\}
\end{equation}

It turns out that this gives the remaining eigenvectors of $\mathbf{X^TX}$, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the loading vectors are eigenvectors of $\mathbf{X^TX}$.

The $k$th principal component of a data vector $\mathbf{x}_{(i)}$ can therefore be given as a score $t_{k(i)} = \mathbf{x}_{(i)} \cdot \mathbf{w}_{(k)}$ in the transformed co-ordinates, or as the corresponding vector in the space of the original variables, $\{\mathbf{x}_{(i)} \cdot \mathbf{w}_{(k)}\} \mathbf{w}_{(k)}$, where $\mathbf{w}_{(k)}$ is the $k$th eigenvector of $\mathbf{X^TX}$.

The full principal components decomposition of $\mathbf{X}$ can therefore be given as
\begin{equation}
    \mathbf {T} =\mathbf {X} \mathbf {W}
\end{equation}

where $\mathbf{W}$ is a $p$-by-$p$ matrix whose columns are the eigenvectors of $\mathbf{X^TX}$. The transpose of $\mathbf{W}$ is sometimes called the whitening or sphering transformation.

See appendix \ref{code:PCA} for the complete python code. 

\paragraph{\textbf{Technique 2: Deep Learning Approach: Auto Encoder}}

~\smallskip

An autoencoder is an artificial neural network used for unsupervised learning of efficient codings. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data. Some of the most powerful AI in the 2010s involves stacking sparse autoencoders in a deep learning network.

Architecturally, the simplest form of an autoencoder is a feedforward, non-recurrent neural network very similar to the multilayer perception (MLP), having an input layer, an output layer and one or more hidden layers connecting them, but with the output layer having the same number of nodes as the input layer, and with the purpose of reconstructing its own inputs (instead of predicting the target value $Y$ given inputs $X$. Therefore, autoencoders are unsupervised learning models.

An autoencoder always consists of two parts, the encoder and the decoder, which can be defined as transitions $\phi$  and $\psi$ such that:
\begin{gather}
\phi :{\mathcal {X}}\rightarrow {\mathcal {F}} \notag\\
\psi :{\mathcal {F}}\rightarrow {\mathcal {X}} \notag\\
\phi ,\psi ={\underset {\phi ,\psi }{\operatorname {arg\,min} }}\,\|X-(\psi \circ \phi )X\|^{2}
\end{gather}

In the simplest case, where there is one hidden layer, the encoder stage of an autoencoder takes the input $\mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}$ and maps it to $\mathbf {z} \in \mathbb {R} ^{p}={\mathcal {F}}$:
\begin{equation}
    \mathbf {z} =\sigma (\mathbf {Wx} +\mathbf {b} )
\end{equation}

This image $\mathbf {z}$  is usually referred to as code, latent variables, or latent representation. Here, $\sigma$  is an element-wise activation function such as a sigmoid function or a rectified linear unit. $\mathbf {W}$  is a weight matrix and $\mathbf {b}$  is a bias vector. After that, the decoder stage of the autoencoder maps $\mathbf {z}$  to the reconstruction $\mathbf {x'}$  of the same shape as $\mathbf {x}$ :
\begin{equation}
    \mathbf {x'} =\sigma '(\mathbf {W'z} +\mathbf {b'} )
\end{equation}


where $ \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} $ for the decoder may differ in general from the corresponding $ \mathbf {\sigma } ,\mathbf {W} ,{\text{ and }}\mathbf {b} $ for the encoder, depending on the design of the autoencoder.

Autoencoders are also trained to minimise reconstruction errors (such as squared errors):
\begin{equation}
     {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}
\end{equation}

where $\mathbf {x}$  is usually averaged over some input training set.

If the feature space $\mathcal {F}$ has lower dimensionality than the input space $\mathcal {X}$, then the feature vector $\phi (x)$ can be regarded as a compressed representation of the input $x$. If the hidden layers are larger than the input layer, an autoencoder can potentially learn the identity function and become useless. However, experimental results have shown that autoencoders might still learn useful features in these cases.

\subsubsection{Linear Regression (LR) for Principle Component}
After we choose the number of components to express the use or trends-in-use according to the U.S. Census socio-economic data, we apply linear regression to test if the components we choose can give a satisfying result that is close to the reports given by the NFLIS data. In this way, we can a score of this linear regression. The closer it approaches $1$, the better the components show the fact.

Given a data set $\left \{ y_{i}, x_{i1}, \cdots , x_{ip} \right \}_{i = 1}^{n}$ of n statistical units, a linear regression model assumes that the relationship between the dependent variable $y$ and the $p$-vector of regressors $x$ is linear. This relationship is modeled through a disturbance term or error variable $ε$ , an unobserved random variable that adds ``noise'' to the linear relationship between the dependent variable and regressors. Thus the model takes the form  as the following shows.

\begin{equation}
    y_i = \beta_{0}1+ \beta_{1}x_{i1}+\cdots +\beta_{p}x_{ip} + \varepsilon _{i} = x_{i}^{T}\beta + \varepsilon_{i}
\end{equation}

The superscript $T$ denotes the transpose, so that $x_{i}^{T}\beta$ is the inner product between vectrs $x_{i}$ and $\beta$. 

Often these n equations are stacked together and written in matrix notation as the following.

\begin{equation}
    y = X\beta + \varepsilon
\end{equation}

\begin{itemize}
\item[-] $y$ is a vector of observed values $y_{i} ( i = 1, \cdots, n )$ of the variable called the regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable. This variable is also sometimes known as the predicted variable. $X$ may be seen as a matrix of row-vectors 
\item[-] $x_{i}$ or of n-dimensional column-vectors $X_{j}$, which are known as regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables (not to be confused with the concept of independent random variables).
 \item[-] $\beta$ is a $( p + 1 )$-dimensional parameter vector, where $\beta _{0}$ is the intercept term (if one is included in the model—otherwise $\beta$ is $p$-dimensional). Its elements are known as effects or regression coefficients (although the latter term is sometimes reserved for the estimated effects).
\item[-] $\varepsilon$ is a vector of values $\varepsilon _{i}$. This part of the model is called the error term, disturbance term, or sometimes noise (in contrast with the ``signal'' provided by the rest of the model).

\end{itemize}

\subsubsection{Results for Dimension Reduction and Linear Regression}\label{sec:DRLR}

We have selected the most important factors from the $596$ dimensions that the U.S. Census socio-economic data that provides by dimensional reduction. And then, we use linear regression to test our selection and get a score for each selection. In this way, we can get a satisfying set of components. 

According to PCA, we can get the variance ratio, which shows the ratio of the variance value of each principal component to the total variance value after dimension reduction. The larger the value of the variance ratio, the more important the principal component. Actually, the more components we take into account, the larger variance ratio that we can get. However, what we pursue is not only the completeness characterization of the fact ,but also the simplification of the vast dimension. So we need to get an appropriate set of components that can characterize all the information as well as simplify this problem.

According to LR, we can get a score for the fitting. Actually, the score is the $R^{2}$ of linear regression results and the fact data provided by the NFLIS Data, which shows how closer the results we get from the components reach the fact. The closer it approaches $1$, the better our fitting according to the chosen components.

In addition, to get a good result, we also need to select the way to deal with the data given by the U.S. Census socio-economic data. So we apply the PCA as well as LR to the these data in four ways, and you can see appendix \ref{code:PCA_plot} for the complete python code. Here we show the results by diagrams:

~\smallskip

(1) Analyze all the data over the years ranging from 2010 to 2016 of the five states respectively, and the results are shown in Figure \ref{fig:pca_state}.

    \begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/pca_state_ratio.eps}
        \subcaption{The ratio of data retained after dimensional reduction among five states in 2010-2016 against different components}
        \label{fig:pca_state_ratio}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/pca_state_score.eps}
        \subcaption{The $R^2$ of linear regression results among five states in 2010-2016 against different components }
        \label{fig:pca_state_score}
    \end{subfigure}
    \caption{PCA and LR results of data among five states in 2010-2016 against different components}\label{fig:pca_state}
\end{figure}

From (a) in Figure \ref{fig:pca_state}, we can see the curve for a single state raises as the number of the components increases. When we choose four components to describe this problem, the superimposed explained variance ratio is almost close to 1. That is to say that under this circumstance, the results that we get from the fitting is very close to the fact. 

From (b) in Figure \ref{fig:pca_state}, we can see the curves for the five years are almost stable as the number of the components increase, except some special cases. As for one component for the WV state, the low coefficient of determination ascribes that there is only one component to characterize the problem. As for nine components for the OH state, the low coefficient of determination may attribute to over-fitting. 

~\smallskip

(2) Analyze the data of 2010 of the five states respectively, and the results are shown in Figure \ref{fig:pca_state_2010}. 
    \begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/pca_state_2010_ratio.eps}
        \subcaption{The ratio of data retained after dimensional reduction among five states in 2010 against different components}
        \label{fig:pca_state_2010_ratio}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/pca_state_2010_score.eps}
        \subcaption{The $R^2$ of Linear Regression results among five states in 2010 against different components}
        \label{fig:pca_state_2010_score}
    \end{subfigure}
    \caption{PCA and LR results of data among five states in 2010 against different components}\label{fig:pca_state_2010}
\end{figure}

From (a) in Figure \ref{fig:pca_state_2010}, we can see the ratio shows the similar characteristics to (a) in Figure \ref{fig:pca_state}, which means an excellent characterization of this problem with only four components. But when it comes to the evaluation part, from (b) in Figure \ref{fig:pca_state_2010}, we can directly see that the fitting is not as stable as the last one.

~\smallskip

(3) Analyze the data of all the five states of 2010 to 2016 respectively, and the results are shown in Figure \ref{fig:pca_year}.
    \begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/pca_year_ratio.eps}
        \subcaption{The ratio of data remained after dimensional reduction among five states in 2010-2016}
        \label{fig:pca_year_ratio}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/pca_year_score.eps}
        \subcaption{The $R^2$ of Linear Regression results of data in 2010-2016 against different aggregations}
        \label{fig:pca_year_score}
    \end{subfigure}
    \caption{PCA and LR results of data in 2010-2016 against different aggregations}\label{fig:pca_year}
\end{figure}

From (a) in Figure \ref{fig:pca_year}, we can see the curve for a single year raises as the number of the components increases almost in the same way. However, the superimposed explained variance ratio gets close to 1 when the number of the components raises to 6 or more, which indicates that it does not do well in simplification. Moreover, the coefficient of determination that is shown in (b) of Figure \ref{fig:pca_year} is unstable.

~\smallskip

(4) Analyze the data of the state PA of 2010 to 2016 respectively, and the results are shown in Figure \ref{fig:pca_year_PA}.
    \begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/pca_year_PA_ratio.eps}
        \subcaption{The ratio of data after dimensional reduction in PA in 2010-2016}
        \label{fig:pca_year_PA_ratio}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figure/pca_year_PA_score.eps}
        \subcaption{The $R^2$ of Linear Regression results in PA over years against different aggregations}
        \label{fig:pca_year_PA_score}
    \end{subfigure}
    \caption{PCA and LR results of data in PA in 2010-2016 against different aggregations}\label{fig:pca_year_PA}
\end{figure}

From (a) in Figure \ref{fig:pca_year_PA}, the curve shows similar characteristics to the above ones and also does well in characterize through only few components. But (b) of Figure \ref{fig:pca_year_PA} also shows its instability.

From the visualization and analysis that we have shown above, it can be concluded that when we choose all the data over the years ranging from 2010 to 2016 of the five states respectively, we can gain a great, simple, and stable dimensional reduction with only 4 components.

For example, we select the U.S. Census socio-economic data of KY state ranging from 2010 to 2016. In PCA, the variance ratio reaches a threshold of about 0.01 after the fourth dimension. The first four variance ratios are 
\begin{center}
    $0.59396827$, $0.28748557$, $0.02754973$, $0.02732665$.
\end{center}

From these four dimension, we can get the score of about $0.92$ by linear regression, which shows a good result of the dimension reduction. 

\subsubsection{Results for Optimizied SOS Model}
Based on the results of Dimension Reduction and Linear Regression in section \ref{sec:DRLR}, we add these four principal components into our BPNN model for modification. With this Optimizied SOS Model, we do as section \ref{sec: Implementation} do and get the Table \ref{tab:2008new} and Table \ref{tab:2019new}

\begin{table}[H]
    \caption{ Results Table for Optimizied SOS Model}
     \begin{subtable}[t]{0.45\linewidth}
     \centering
    \caption{Hydrocodone identification counts in 2008 in  Kentucky (KY) using modified SOS model}
    \label{tab:2008new}
    \begin{tabular}{|c|c|}
        \hline
County& Drug Reports  \\\hline
JEFFERSON&1158.60\\
LAUREL&103.17\\
PERRY&98.59\\
BELL&86.35\\
HARLAN&85.24\\
PULASKI&78.45\\
FAYETTE&77.75\\
WHITLEY&68.65\\
PIKE&68.62\\
FLOYD&61.29\\
\hline
    \end{tabular}
\end{subtable}\hfill
 \begin{subtable}[t]{0.45\linewidth}
    \centering
    \caption{Buprenorphine identification counts in 2019 in Ohio (OH) using modified SOS model}
    \label{tab:2019new}
    \begin{tabular}{|c|c|}\hline
      County& Drug Reports  \\\hline  
FRANKLIN&240.30\\
HAMILTON&155.90\\
MONTGOMERY&140.92\\
CUYAHOGA&139.51\\
FAIRFIELD&98.06\\
CRAWFORD&89.35\\
STARK&82.62\\
GUERNSEY&81.52\\
LAKE&78.01\\
COLUMBIANA&75.36\\
\hline
    \end{tabular}
\end{subtable}
\end{table} 

Finally, we apply the model to all of the five states so that we can analysis the coefficient of determination between the origin model and the new model with PCA optimization. The results are shown in Table \ref{tab:final}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        State & Substance & Origin Model & Optimized Model \\\hline
        KY & Hydrocodone & 0.8957447139498953 & 0.9120307701827662 \\
        OH & Hydrocodone & 0.8501898003702214 & 0.8641988375370137 \\
        PA & Hydrocodone & 0.9319499913029298 & 0.9325942856975605 \\
        VA & Hydrocodone & 0.8191016628092015 & 0.8374162880363936 \\
        WV & Hydrocodone & 0.6797258899970731 & 0.7051639551372615 \\
        KY & Buprenorphine & 0.7697562356106809 & 0.7998899587972155 \\
        OH & Buprenorphine & 0.8720902736185301 & 0.8807783772009824 \\
        PA & Buprenorphine & 0.9379855180669447 & 0.9418626513609050 \\
        VA & Buprenorphine & 0.8720028255610893 & 0.8832633164948145 \\
        WV & Buprenorphine & 0.12985720067610063 & 0.14259618407280017 \\
        \hline 
    \end{tabular}
    \caption{The coefficient of determination of the origin model vs the new model with PCA optimization.}
    \label{tab:final}
\end{table}

We can found that the optimization have a good effect on the value of $R^2$, however, on the last row, Buprenorphine in WV State, the model doesn't fit the data well. This is probably because of the lack of data (only 234 data, and most of them are relatively small) of this kind of opioid in this state.

\subsection{Strategy Program for the Opioid Crisis}\label{Sec:Strategy}

\subsubsection{SSD (Supply Control, Spread Control, and Demand Control) }
	
	Understanding that certain levels of drug use are inevitable, so we should focus on minimizing adverse effects associated with drug use rather than stopping the behavior itself. In the context of the opioid epidemic, we propose the \textbf{Strategy Program ``SSD'' (Supply Control, Spread Control, Demand Control)} that are designed to improve health outcomes and reduce overdose deaths.
	
    It can be divided into three aspects:
\paragraph{\textbf{Supply Control}}

~\smallskip

\begin{itemize}
    \item Requiring manufacturers of long-acting opioids to sponsor educational programs for prescribers. Through these educational programs, we can control opioid supply and help deter off-label and over-prescribing, making a threshold to those who want to abuse the opioids~\cite{FUSTER2018487}.
    \item We need to strengthen cooperation with countries such as Colombia and Mexico, to crack down on cross-border drug crimes.
\end{itemize}

\paragraph{\textbf{Spread Control}}

~\smallskip

\begin{itemize}
    \item Strengthen public health data reporting and collection to improve the timeliness and specificity of data and to inform a real-time public health response as the opioid epidemic evolves，so that our Spreading Model can forecast the opioid epidemic more precisely~\cite{AWSARE2019}.
    \item Strengthening Public Health Data and Reporting. Timely, high-quality data help both public health officials and law enforcement understand the extent of the problem and how it is evolving, develop interventions, focus resources where they are needed most, and evaluate the success of prevention and response efforts. 
\end{itemize}

\paragraph{\textbf{Demand Control}}
~\smallskip

\begin{itemize}
    \item Advance the practice of pain management to enable access to high-quality, evidence-based pain care that reduces the burden of pain for individuals, families, and society while also reducing the inappropriate use of opioids and opioid-related harms~\cite{SEIGERMAN201928}.
    \item Only by making opioid control and management more rigorous and formal can we effectively combat illicit drug abuse.
\end{itemize}

\subsubsection{Effectiveness of our Strategy Program}
\paragraph{\textbf{Supply Control}}

~\smallskip

Corresponding to our SOS model, controlling Supply means setting a \textbf{threshold function} for input in our model. Once certain opioid reaches its threshold, it would be hindered strongly by FBI, which makes sense in reality. Therefore, the Corresponding output will decrease significantly.
\paragraph{\textbf{Spread Control}}

~\smallskip

Corresponding to our SOS model, controlling Spread means setting a \textbf{punishing function }in our model based on the distance between each two counties. The farther apart the two counties are, the greater the spreading costs they pay, which also makes sense in reality.Therefore, the Corresponding output will decrease significantly.
\paragraph{\textbf{Demand Control}}

~\smallskip

Corresponding to our SOS model, controlling Demand means setting a \textbf{resisting function} in our every neuron, since people are less convenient to have access to opioids. By this way, we can simulate actual situations, which prove to be effective in countering opioids crisis.Therefore, the Corresponding output will decrease significantly.
\section{Model Analysis}\label{Sec-Analysis}

\subsection{Sensitivity Analysis}
\begin{itemize}
    \item In our Back-Propagation Neutral Network (BPNN) Model, there is no parameter, but a sparse-connected hidden layer between the input layer and output layer to solve this problem. Thus, the robustness of our model is only dependent on data scale.
    \item In our Principal Component Analysis (PCA) and Linear Regression progress, we did the sensitivity analysis in section \ref{sec:DRLR}.
    \item As for Strategy Program ``SSD'', we have no extra time to do sensitivity analysis, but we did a comprehensive theoretical analysis concerning every possible factors.
\end{itemize}

\subsection{Strengths and Weaknesses}
\subsubsection{Strengths}
\begin{itemize}
\item \textbf {Effective and Uniform Data Extraction:} We uniformly use the library pandas in python  to evaluate and filter the vast data for each problem. In this way, we apply the most appropriate data to each part and get a better result effectively.

\item \textbf{High Characterization and Generalizability:} We design the model by BPNN that fits not only all the data but also each state of this problem.

\item \textbf{Innovative Modeling with BPNN:} We apply BPNN to construct our model with highly self-learning and highly self-adapting abilities, which is good for us to solve this problem.

\item \textbf{Bilateral Prediction by BPNN:} Our model can predict not only when and where specific concerns may occur but also the source of the crisis.

\item \textbf{Alternative Methods for Dimensionality Reduction:} We introduced two methods: PCA and AE for dimensionality reduction and compare their results.

\item \textbf{Prudential Test of the Model:}After the dimensionality reduction, we apply the components that we get to LR and get a fitting result. Through comparison, we evaluate whether our dimensionality reduction is good.

\item \textbf{Quantified and Rational Goals:} We set quantified goals strictly based on optimization theory.
\end{itemize}


\subsubsection{Weaknesses}
\begin{itemize}
\item \textbf{No Verification of Raw Data:} We have no guarantee of the accuracy of given data , and the data is not complete.
\item \textbf{No Involvement of Other States:} We do not consider states other than OH, KY, WV, VA, and PA, due to the lack of relevant data.
\item \textbf{No Accurate Geographical Information: } We use the the official coordinates of counties to represent its geographical location. And we have no guarantee of the accuracy of the data  from the Search (Nominatim) API~\cite{bing}.
\end{itemize}

\section{Conclusion} \label{Sec-Conclusion}
In this paper,we propose a novel framework called \textbf{SOS} (Spreading Model, Optimization, and Strategy Program for the Opioid Crisis). First we create a \textbf{Back Propagation Neutral Network  (BPNN) Model} to describe the spread and characteristics of the opioid incidents in and between the five states over time. With this model we identified possible locations where specific opioid use might have emerged in each of the five states. Second we adopt two different techniques: \textbf{Principal Component Analysis (PCA)} and \textbf{Deep Learning Approach: Auto-Encoder}, to extract principal components from the U.S. Census Socio-Economic data. After testing the validity of these components with \textbf{Linear Regression},  we add to our Spreading Model for modification. Third we identify a \textbf{Strategy Program ``SSD''} for countering the opioid crisis. Tested by our model, these strategies are proven to be significantly effective in overcoming the opioid crisis. Finally, we conduct sensitivity analysis of some parameters in our model and discuss the strengths and weakness of our work.

\newpage
\section*{MEMORANDUM} 

\noindent\textbf{To:} Chief Administrator, DEA/NFLIS Database

\noindent\textbf{From:} Team \# 1920446

\noindent\textbf{Date}: Jananry 29, 2019

\noindent\textbf{Subject:} The Opioids Model: Analysis for Spread, Idetification for Source, Prediction for Future, and Strategy For Control
\addcontentsline{toc}{section}{MEMORANDUM}

Honorable Chief Administrator, DEA/NFLIS Database, 

Currently, the opioid crisis has overwhelmed the whole America, urging us to lay great emphasis on controlling the spreading opioid incidents. As the person in charge, our team has made a comprehensive study on countering opioid crisis based on the reported synthetic opioid and heroin incidents (cases) from 2010 to 2017 in and between the five states: Ohio (OH), Kentucky (KY), West Virginia (WV), Virginia (VA), and Pennsylvania (PA). We further analyze the U.S. Census Socio-Economic data to modify the initial model. Here come our study results:

\subsection*{Spreading Model }
\addcontentsline{toc}{subsection}{Spreading Model }
Here we utilize a Back-Propagation Neutral Network (BPNN) to build this model, characterizing the basic evolving and spreading trends of the opioid and heroin incidents. Thus, we forecast the opioid accidents happened in the following two years, and find that if we do not take effective measures at once, most of opioids incidents will increase sharply, which demonstrates the urgence of solving the opioid abuse problem right now.

In order to identify any possible locations where specific opioid use might have started, we adopt Outlier Detection and identify the outliers as the trending locations.

\subsection*{Optimization}
\addcontentsline{toc}{subsection}{Optimization}
 In this part, we combine Principal Component Analysis (PCA) and Learning Approach Auto-Encoder two techniques to analyze the U.S. Census Socio-economic data provided. We successfully select the most important $4$ factors from the $596$ dimensions in data. And then, we use Linear Regression to test our selection and get a score for each selection, Which shows the similarity to the fact. In this way, we get a satisfying set of components added to our model for modification. 

\subsection*{Strategy Program}
\addcontentsline{toc}{subsection}{Strategy Program }
Eventually, according to the results above, we figure out a comprehensive and feasible Strategy Program called ``SSD''. The whole strategy program can be divided into three parts: Supply Control, Spread Control, and Demand Control. After being tested by our promoted model, these strategies are proven to be significantly effective in controlling and preventing the opioid crisis. Concrete Strategies are as follows:
\begin{itemize}
    \item \textbf{Supply Control:} To require manufacturers of long-acting opioids to sponsor educational programs for prescribers, making a threshold to those who want to abuse the opioids.To strengthen cooperation with countries such as Colombia and Mexico, to crack down on cross-border drug crimes.
    \item \textbf{ Spread Control:} To strengthen public health data reporting and collection to improve the timeliness and specificity of data and to inform a real-time public health response as the opioid epidemic evolves.
    \item \textbf{Demand Control:} To advance the practice of pain management to enable access to high-quality, evidence-based pain care that reduces the burden of pain for individuals, families, and society while also reducing the inappropriate use of opioids and opioid-related harms. To make opioid control and management more rigorous and formal for effectively combating illicit drug abuse.
\end{itemize}


The above is the summary of our study. We sincerely hope that it will provide you with useful information.

Thanks!

\newpage

\bibliographystyle{IEEEtran}
\bibliography{newrefs}
	
\ \newpage

\begin{appendices}

\section{Python Code}

\subsection{map.py}\label{code:map}
\inputminted{python}{../src/map.py}

\subsection{geocode.py}\label{code:geocode}
\inputminted{python}{../src/geocode.py}

\subsection{distance.py}\label{code:distance}
\inputminted{python}{../src/distance.py}

\subsection{count.py} \label{code:count}
\inputminted{python}{../src/count.py}

\subsection{bpnn.py} \label{code:bpnn}
\inputminted{python}{../src/bpnn.py}

\subsection{PCA.py} \label{code:PCA}
\inputminted{python}{../src/PCA.py}

\subsection{PCA\_plot.py} \label{code:PCA_plot}
\inputminted{python}{../src/PCA_plot.py}



\end{appendices}
\end{document}

%%
%% This work consists of these files mcmthesis.dtx,
%%                                   figures/ and
%%                                   code/,
%% and the derived files             mcmthesis.cls,
%%                                   mcmthesis-demo.tex,
%%                                   README,
%%                                   LICENSE,
%%                                   mcmthesis.pdf and
%%                                   mcmthesis-demo.pdf.
%%
%% End of file `mcmthesis-demo.tex'.
